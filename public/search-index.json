[{"slug":"2020-plan","category":"blog","title":"The 2020 plan","tags":["mentalhealth"],"body":"\nAt the beginning of 2019 I set about creating a list of all the things I wanted to learn. It was huge! I was kidding my self if I thoughtI was ever going to be able to do all the things I had written down.\n\nObviously life happened, events occurred that I could not have planned for and my long list remained largely unfinished. I think I probably \"completed\" about 3 of the 10 tasks I had set myself. That is not to say that I failed in 2019. Far from it. But where I directed my time had to shift. I interviewed for and got a new job, something I was not planning on and something that consumed an awful lot of my time. But it was worth the input and sacrificing some of the tasks I had set at the beginning of the year.\n\n## Whats in store for 2020\n\n2020 is already shaping up to be a good year. I am 5 weeks into my new role, which I love. We have already recorded the first episode of Salted Bytes and I have a new plan in place.\n\nThis year, I am gonna be more realistic. I am gonna concentrate on few things in the hope that I will be able to finish. But if I don't, then that is also OK.\n\n### Salted Bytes\n\nWe are planning on stepping things up a gear in 2020. The plan is to record 2 episodes a month and to try and get more guests (if you fancy coming on the show please get in touch). I am also planning on trying to write 2 articles a month. These will be as varied as ever but will still center around the software industry.\n\n### Health\n\nIn the last year I have been using the mornings before work to swat up on my current interests. But this year I am going to use this time to exercise a little self care. It is all to easy to neglect your physical and mental health and this takes its tole.\n\nSo this year I will be exercising, doing yoga and practising a little mindfulness in the hope that an increase in mental and physical health will help me become a better engineer.\n\n### Programming\n\nSoftware is a tough one. I just want to learn everything! It's all so interesting but I will be taking some learnings from last year, I think it is far too difficult to plan a years worth of learning and exploration. Stuff changes so fast and my interests and where I need to concentrate time does too. So I am going to approach it in a different way. I will only be looking 3 months ahead in the hope that this is more maintainable. I still a rough idea of what I want to learn but I a realistic in the knowledge that this will change.\n\nHere are a few things that I will be concentrating on:\n\n- Getting better at the stuff I already know.\n- Application security.\n- Making stuff - Hardware and software.\n\nThats it! A start contrast from last years plan but one that I feel is more achievable and one that is less likely to make me feel like I have failed. What do you have have in store for 2020.\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know.\n\nCheck out our software focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"},{"slug":"cinch-layout","category":"blog","title":"CINCH ‚Äî A layout library for React Native","description":"I am super pumped to talk about the release of CINCH, a layout library for React Native.","tags":["reactnative","react","javascript"],"body":"\nI am super pumped to talk about the release of CINCH, a layout library for React Native.\n\n> CINCH‚Ää-‚ÄäMaking React Native layouts a cinch\n\nThe aim of the library is to take away the complexities of flexbox layouts in React Native. Cinch used Styled Components as a base for build the styled layout components and as such is a peer dependency of Cinch.\n\nThe library is inspired by Hedron and follows a similar API. I had been in discussion with Garet, Hedrons creator about adding React Native support to Hedron and having taken a look, I decided to create a stand-alone library. I did not want to bloat the Hedron package adding complexity to the build config. Also, the React Native implementation would not use a lot of the current functionality of Hedron.\nExample\n\n![Alt Text](https://thepracticaldev.s3.amazonaws.com/i/x9tel61ko2y00a346p0l.png)\n\nLet have a look at a simple example.\n\nCreate a new project\n\n`react-native init cinchExample && cd $_`\n\nNow let's install Cinch.\n\n`yarn add cinch-layout`\n\nOpen your project in your favourite editor and replace the contents of App.js with the following:\n\n```javascript\nimport * as React from \"react\"\nimport { SafeAreaView, Text } from \"react-native\"\nimport { CinchProvider, CinchBounds, CinchBox } from \"./src\"\n\nexport default class App extends React.Component {\n  render() {\n    return (\n      <SafeAreaView style={{ flex: 1, backgroundColor: \"white\" }}>\n        <CinchProvider>\n          <CinchBounds flex={1} debug flexDirection=\"vertical\">\n            <CinchBox debug style={{ width: \"50%\" }}>\n              <Text>Hello</Text>\n            </CinchBox>\n\n            <CinchBox flex={2} debug={true}>\n              <Text>Hello</Text>\n            </CinchBox>\n\n            <CinchBox flex={1} debug={true}>\n              <Text>Hello</Text>\n            </CinchBox>\n          </CinchBounds>\n          <CinchBounds>\n            <CinchBox debug valign={\"center\"} halign=\"right\">\n              <Text>Hello</Text>\n            </CinchBox>\n            <CinchBox debug style={{ marginHorizontal: 20 }}>\n              <Text>Hello</Text>\n            </CinchBox>\n          </CinchBounds>\n        </CinchProvider>\n      </SafeAreaView>\n    )\n  }\n}\n```\n\nFinally, start your React Native project.\n\nThis simple example shows you how easy it is to create complex flexbox layouts.\n\nThe debug prop will add borders to each Cinch component giving you a better visual display of the View components.\n\nEach Cinch component will default to flex: 1 but you can easily change the flex value by adding the `flex={}` props.\n\nNeed to change the `flexDirection`? Just use the `flexDirection={}` prop. Aligning the child elements could not be easier. Just add a `valign={}` or `halign={}` prop and Cinch will work out the placement based on the `flexDirection` props.\n\nEach Cinch component also accepts the React Native style props and will pass those styles to the correct component.\n\nAnd that is about it. The package is still new and will have some issues\n\n# Thanks for reading üôè\n\nCheck out our software focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"},{"slug":"content-based-recommendations","category":"blog","title":"Build a Content-based recommendation engine in JS","description":"Want to build a content based recommendation engine? Look no further.","tags":["machinelearning","javascript"],"body":"\nMachine learning has been on my radar for a long time but I have never really knuckled down and actually started learning it. That is until recently. I am a serial learner and with nothing lined up I decided to tackle some machine learning. I set myself the task of creating a recommendation engine. We interact with these ever day, through social media and online shopping as well as so many other places. I used a simple dataset from the web that consisted on 20 images with the results from a Google Vision API request. My goal was to recommend other images from the collection when a single image is selected.\n\nI realise that Python would probably have been a better choice of language for this task but I know Javascript very well and did not want the extr burden of having to piece together the engine in a language I am not 100% comfortable with.\n\nAccording to Wikipedia, a content-based recommendation engine is:\n\n> ‚ÄúRecommender systems or recommendation systems (sometimes replacing ‚Äúsystem‚Äù with a synonym such as a platform or engine) are a subclass of information filtering system that seeks to predict the ‚Äòrating‚Äô or ‚Äòpreference‚Äô that user would give to an item.‚Äù\n\nRecommendation engines are active filtering systems that personalise the information coming to a user based on information known about a user. In our case, this information is the image initially selected and the data that was returned from Google Vision.\n\nBest the end of this article we will be able to recommend a user more images based on their initial image selection.\n\n## The pros and cons\n\nBefore we run through how. Let's talk about why. There is a reason this type of engine is so popular but there will be reasons not to use it as well.\n\n### Pros\n\n- Unlike other methodologies, content-based filtering does not need the data of other users, since the recommendations are specific to the user. This avoids the issue of cold starts where there is limited data\n- The model captures the specific interests of the users and so can recommend niche items that may not be popular with other users\n\n### Cons\n\n- The model can only make recommendations based on existing interests. This limits the recommendations to known interests, stopping the broadening of the user's interests\n- You are reliant on the accuracy of the labels\n- Does not take into account a user's quirks. They like something but only in a very specific circumstance.\n\n## How do content-based recommendation engines work\n\nA content-based recommendation engine works with data that a user provides (in our case, selecting an image). Based on this data we can make suggestions to the user.\n\nIn our case, our script will progress through the following steps:\n\n1. Training\n\n- Format data into a usable state\n- Calculate TF-IDF and create vectors from the formatted docs\n- Calculate similar documents\n\n2. Use trained data to make a recommendation based on the user's image selection.\n\nBefore we start writing our recommendation engine we need to talk about a few key concepts. Namely, how are we going to decide what data to recommend?\n\nThe concepts of Term Frequency (TF) and Inverse Document Frequency (IDF) are used to determine the relative importance of a term. With that, we can use the concept of cosine similarity to determine what to recommend. We will discuss these throughout the article.\n\nTF is simply the frequency a word appears in a document. The IDF is the frequency of a term in a whole corpus of documents. It signifies the rarity of a word and helps to boost the score of rarer terms. TD-IDF is used because it takes into account not only the isolated term but also the term within the whole corpus of documents. This model combines how important the word is in the document (local importance), with how important the word is int the whole corpus (global importance)\n\nCosine similarity is a metric used to determine the similarity of documents irrespective of size. Mathematically it is measuring the cosine angle between 2 vectors. In our context, the vectors will be objects containing the term as the key and the TF-IDF as the value. The value is also referred to as the vector's magnitude.\n\n### 1. Training\n\n![training](https://media.giphy.com/media/1iTH1WIUjM0VATSw/giphy.gif)\n\nThe first step in \"training\" our engine is to format the data into a structure that is usable and easy to manage. The labels data that comes back from Google Cloud Vision looks something like this:\n\n```json\n{\n  \"1.jpg\": [\n    {\n      \"locations\": [],\n      \"properties\": [],\n      \"mid\": \"/m/0c9ph5\",\n      \"locale\": \"\",\n      \"description\": \"Flower\",\n      \"score\": 0.9955990314483643,\n      \"confidence\": 0,\n      \"topicality\": 0.9955990314483643,\n      \"boundingPoly\": null\n    },\n    {\n      \"locations\": [],\n      \"properties\": [],\n      \"mid\": \"/m/04sjm\",\n      \"locale\": \"\",\n      \"description\": \"Flowering plant\",\n      \"score\": 0.9854584336280823,\n      \"confidence\": 0,\n      \"topicality\": 0.9854584336280823,\n      \"boundingPoly\": null\n    },\n    [...]\n  ]\n}\n```\n\n#### 1.a Formatting\n\nFor the purpose of this exercise, we are only concerned with the top-level key of the object (`1.jpg`) and the `description` of each of the objects in the array. But we want all of the descriptions in a single string. This will allow us to process them more easily later on.\n\nWe want the data to be in an array of objects like this:\n\n```javascript\nconst formattedData = [\n  {\n    id: \"1.jpg\",\n    content:\n      \"flower flowering plant plant petal geraniaceae melastome family geranium wildflower geraniales perennial plant\",\n  },\n]\n```\n\nTo format our data we will run it through the following function. This will return an array of all the data we need to continue training our engine. We use `Object.entries` to allow us to iterate more easily. MDN states that:\n\n> The Object.entries() method returns an array of a given object's own enumerable string-keyed property [key, value] pairs...\n\nWe then loop over the array created bt `Object.entries` plucking the necessary properties and add them to a `desc` array. Finally, we join the contents of the `desc` array and write it to the `content` property. This `formatted` array is our corpus.\n\n```javascript\nconst formatData = data => {\n  let formatted = []\n\n  for (const [key, labels] of Object.entries(data)) {\n    let tmpObj = {}\n    const desc = labels.map(l => {\n      return l.description.toLowerCase()\n    })\n\n    tmpObj = {\n      id: key,\n      content: desc.join(\" \"),\n    }\n\n    formatted.push(tmpObj)\n  }\n\n  return formatted\n}\n```\n\n#### 1.b TF-IDF and Vectors\n\nAs mentioned above the TF is just the number of times a term features in a document.\n\nFor example:\n\n```javascript\n// In the data set below the TF of plant is 3\n{\n  id: '1.jpg',\n  content: 'flower flowering plant plant petal geraniaceae melastome family geranium wildflower geraniales perennial plant'\n}\n```\n\nThe IDF is slightly more complex to work out. The formula is:\n\n![Alt Text](https://thepracticaldev.s3.amazonaws.com/i/s9xr77wjmtvka7eh885o.png)\n\nIn javascript this is worked out with:\n\n```javascript\nvar idf = Math.log(this.documents.length / docsWithTerm)\n```\n\nWe only need to above values (TF and IDF) so we can calculate the TF-IDF. It's simply TF multiplied by the IDF.\n\n```javascript\nconst tdidf = tf * idf\n```\n\nThe next step in our process is to calculate the TF-IDF of our documents and create a vector containing the term as the key the value (vector) as the TF-IDF. We are leaning on `natural` and `vector-object` npm packages to allow us to do this easily. `tfidf.addDocument` will tokenise our `content` property. The `tfidf.listTerms` method lists our new processed documents returning an array of objects containing the TD, IDF and TD-IDF. We are only concerned with the TF-IDF though.\n\n```javascript\n/**\n* Generates the TF-IDF of each term in the document\n* Create a Vector with the term as the key and the TF-IDF as the value\n* @example - example vector\n* {\n*   flowers: 1.2345\n* }\n*/\nconst createVectorsFromDocs = processedDocs => {\n  const tfidf = new TfIdf();\n\n  processedDocs.forEach(processedDocument => {\n    tfidf.addDocument(processedDocument.content);\n  });\n\n  const documentVectors = [];\n\n  for (let i = 0; i < processedDocs.length; i += 1) {\n    const processedDocument = processedDocs[i];\n    const obj = {};\n\n    const items = tfidf.listTerms(i);\n\n    for (let j = 0; j < items.length; j += 1) {\n      const item = items[j];\n      obj[item.term] = item.tfidf;\n    }\n\n    const documentVector = {\n      id: processedDocument.id,\n      vector: new Vector(obj)\n    };\n\n    documentVectors.push(documentVector);\n  }\n```\n\nNow we have an array of objects containing the id of the image (`1.jpg`) as the id and our vector. Our next step is to calculate the similarities between the documents.\n\n#### 1.c Calculating similarities with Cosine similarity and the dot product\n\nThe final step in the \"training\" stage is to calculate the similarities between the documents. We are using the `vector-object` package again to calculate the cosine similarities. Once calculated we push them into an array that contains the image id and all the recommended images from the training. Finally, we sort them so that the item with the highest cosine similarity is first in the array.\n\n```javascript\n/**\n* Calculates the similarities between 2 vectors\n* getCosineSimilarity creates the dotproduct and the\n* length of the 2 vectors to calculate the cosine\n* similarity\n*/\nconst calcSimilarities = docVectors => {\n  // number of results that you want to return.\n  const MAX_SIMILAR = 20;\n  // min cosine similarity score that should be returned.\n  const MIN_SCORE = 0.2;\n  const data = {};\n\n  for (let i = 0; i < docVectors.length; i += 1) {\n    const documentVector = docVectors[i];\n    const { id } = documentVector;\n\n    data[id] = [];\n  }\n\n  for (let i = 0; i < docVectors.length; i += 1) {\n    for (let j = 0; j < i; j += 1) {\n      const idi = docVectors[i].id;\n      const vi = docVectors[i].vector;\n      const idj = docVectors[j].id;\n      const vj = docVectors[j].vector;\n      const similarity = vi.getCosineSimilarity(vj);\n\n      if (similarity > MIN_SCORE) {\n        data[idi].push({ id: idj, score: similarity });\n        data[idj].push({ id: idi, score: similarity });\n      }\n    }\n  }\n\n  // finally sort the similar documents by descending order\n  Object.keys(data).forEach(id => {\n    data[id].sort((a, b) => b.score - a.score);\n\n    if (data[id].length > MAX_SIMILAR) {\n      data[id] = data[id].slice(0, MAX_SIMILAR);\n    }\n  });\n\n  return data;\n```\n\nUnder the hood, the `getCosineSimilarity` method is doing a number of things.\n\nIt generates the dot product, this operation takes 2 vectors and returns a single (scalar) number. It is a simple multiplication of each component in both vectors added together.\n\n```\na = [1.7836, 3]\nb = [4, 0.5945]\n\na.b = 1.7836 * 4 + 3 *0.5945 = 8.9176\n```\n\nWith the dot product calculated we just need to reduce the vector values of each document down to scalar values. This is done by taking the square root of each value multiplied by itself added together. The `getLength` method below is doing this calculation.\n\n```javascript\nconst getLength = () => {\n  let l = 0\n\n  this.getComponents().forEach(k => {\n    l += this.vector[k] * this.vector[k]\n  })\n\n  return Math.sqrt(l)\n}\n```\n\nThe actual cosine similarity formula looks like this:\n\n![Alt Text](https://thepracticaldev.s3.amazonaws.com/i/d4zgus57bjhsu05d2lgp.png)\n\nand in javascript looks like this:\n\n```javascript\nconst getCosineSimilarity = vector => {\n  return this.getDotProduct(vector) / (this.getLength() * vector.getLength())\n}\n```\n\nThe training is complete!!\n\n![Check](https://media.giphy.com/media/l4EpblDY4msVtKAOk/giphy.gif)\n\n## 2. Getting our recommendations\n\nNow we have completed the training phase we can simply request the recommended images from the training data.\n\n```javascript\nconst getSimilarDocuments = (id, trainedData) => {\n  let similarDocuments = trainedData[id]\n\n  if (similarDocuments === undefined) {\n    return []\n  }\n\n  return similarDocuments\n}\n```\n\nThis will return an array of objects containing the recommended images and their cosine similarity score.\n\n```javascript\n// e.g\n;[\n  { id: \"14.jpg\", score: 0.341705472305971 },\n  { id: \"9.jpg\", score: 0.3092133517794513 },\n  { id: \"1.jpg\", score: 0.3075994367748345 },\n]\n```\n\n## Wrap up\n\nI hope you were able to follow along. I learned so much from this exercise and it has really piqued my interest in machine learning.\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know.\n\nCheck out our software focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"},{"slug":"essential-packages-2020","category":"blog","title":"Essential terminal packages in 2020","description":"These are the CLI packages I cant live without.","tags":["productivity"],"body":"\nThis is the start of a new series about maximising your terminal experience. I will write about what I think are essential packages for the CLI, a simple but super effective Vim setup and setting some sane Tmux defaults.\n\nIn this first article, I am going to list some of what I think are essential CLI packages. These are ones I use every day and could not live without. I am leaving Vim and Tmux off this list as they will get there own articles.\n\nI spend all my time in a terminal. This is where I do all my development, hacking and note-taking. Spending so much time there I want it to work for me, providing a buttery smooth experience.\n\nHere is a list of my top CLI packages.\n\n## 1. ZSH and oh-my-zsh:\n\nThis is my chosen shell. It has way too many features to list here but here are a choice few:\n\n- automatic cd: Just type the name of the directory\n- recursive path expansion: For example ‚Äú/u/lo/b‚Äù expands to ‚Äú/usr/local/bin‚Äù. This is not one I use much because I lead on the tools mentioned below.\n- spelling correction and approximate completion: If you make a small mistake typing a directory name, ZSH will fix it for you\n- plugin and theme support: ZSH includes many different plugin frameworks. This is where [`oh-my-zsh`](https://ohmyz.sh/) comings in. Its a community drive framework for managing your ZSH configuration.\n\n![ZSH](/images/posts/favourite-cli/zsh.png)\n\n## 2. FZF\n\n[FZF](https://github.com/junegunn/fzf) is a command-line fuzzy finder. It is super fast, light and works so well with all my other tools.\n\n![fzf](/images/posts/favourite-cli/fzf.png)\n\n## 3. RipGrep (rg)\n\n[RipGrep](https://github.com/BurntSushi/ripgrep) is a stupid fast replacement for grep. It does away with lots of the hard to remember flags that grep requires. By default, it respects your `.gitignore`, and skips hidden files.\n\n## 4. Autojump\n\n[Autojump](https://github.com/wting/autojump) is a super-fast way to navigate your filesystem. It learns the directories you visit most and remembers them for next time.\n\n## 5. HTTPie\n\n[HTTPie](https://httpie.org/) is a user-friendly command line HTTP client. It has JSON support, syntax highlighting and lots more.\n\n## 6. Bat\n\n[Bat](https://github.com/sharkdp/bat) is a drop-in replacement for `cat` but with superpowers. It's highly configurable and supports syntax highlighting.\n\n![Bat](/images/posts/favourite-cli/bat.png)\n\n## 7. exa\n\n[exa](https://the.exa.website/) is a modern replacement for `ls`. It is highly configurable, provides better defaults.\n\n![HTTPie](/images/posts/favourite-cli/exa.png)\n\n## 8. twf\n\n[twf](https://github.com/wvanlint/twf) is a tree view explorer inspired by fzf.\n\n![HTTPie](/images/posts/favourite-cli/twf.png)\n\nYou can find my dotfiles [here](https://github.com/jim-at-jibba/my-dots). This is how I use them, how will you?\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know.\n\nCheck out our software focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"},{"slug":"fix-coc-server-neovim","category":"blog","title":"Fixing coc-tsserver errors due to Deno","tags":["typescript","vim","productivity"],"body":"\nThis is a quick post detailing how to fix issues with [`coc-tsserver`](https://github.com/neoclide/coc-tsserver).\n\nI am a long term Neovim user and feel that the only Intellisense engine to use is [CocVim](https://github.com/neoclide/coc.nvim). It is simply amazing. It breathes so much power into you Vim setup.\n\nI write Typescript daily and so use the [`coc-tsserver`](https://github.com/neoclide/coc-tsserver) plugin. It gives all the power of VSCodes typescript engine in NeoVim. But recently it broke. It was reporting errors when there was nothing wrong. I made sure all my dependencies we up to date and still nothing. All I was seeing was this:\n\nThe error message is: `The error message is: [tsserver 2307] [E] Cannot find module <import-path>`\n\nSomething was wrong.\n\nThankfully someone was having the same issue and had created an issue in the CocVim git repo. It turns out that there is a bug in the `coc-deno` plugin that has some problems with non-deno projects.\n\nThankfully the fix is simple. Disable the deno plugin for non deno projects.\n\nWith in Vim: `:CocList extensions` and find the Deno plugin. Press tab and select `d` to disable it. Restart Vim and all your issues should have disappeared.\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know\n"},{"slug":"geospacial-queries-elastic-search","category":"blog","title":"Geospatial queries in Elasticsearch","description":"Learn how to use a users location in your elastic search queries to only return the most relevent information","tags":["elasticsearch"],"body":"\n## How we use location data provide region specific search results.\n\nThe Candide App is used across multiple countries, each with their own seasons and unique plants. This means that the way we store that data needs to reflect this and be easily searchable on a regionalised basis.\n\nFor most of our search needs, we lean on Elasticsearch. Where we felt it was important we have broken the indexes into regionalised domains (plants, problems etc). But in some cases, we felt that splitting the indexes was not the right thing to do. As our user base grows the need to regionalise the results of the indexes that are not split by region become ever greater. Being able to provide search results that are appropriate for you location provides a far better user experience.\n\n## Setting out geo_point field mapping\n\nFor us to be able to make geospatial queries against out Elasticsearch index we need to correctly set up the field to be queried when creating our index. This is done by changing the type that is specified when setting the mappings for the index. The type in question is the _geo_point_ type. According to the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/index.html), when setting fields to a `geo_point` types we can:\n\n1. find geo-points within a bounding box, within a certain distance of a central point or a polygon\n2. aggregate documents geographically by distance\n3. integrate the distance into the relevance score\n4. sort documents by distance\n\nNow that we know some of the benefits of using geospatial queries, let look at the request that will set the _geo_point_ within our index' mappings. The snippet below will change the type of the _location_ field to be _geo_point_.\n\n```json\n  PUT my_index\n  {\n    \"mappings\": {\n      \"properties\": {\n        \"location\": {\n          \"type\": \"geo_point\"\n        }\n      }\n    }\n  }\n```\n\n## Adding some data to our index\n\nNow that we have set up our index correctly, we need to add some data. Geo-point fields can accept data in 5 ways. Each of these can be seen below. The one you choose will depend on the needs of your application. Take note of the format of the lat, lon, in some instances, the order is switched.\n\n```json\nPUT my_index/_doc/1\n{\n  \"text\": \"Geo-point as an object\",\n  \"location\": {\n    \"lat\": 41.12,\n    \"lon\": -71.34\n  }\n}\n\nPUT my_index/_doc/2\n{\n  \"text\": \"Geo-point as a string\",\n  \"location\": \"41.12,-71.34\"\n}\n\nPUT my_index/_doc/3\n{\n  \"text\": \"Geo-point as a geohash\",\n  \"location\": \"drm3btev3e86\"\n}\n\nPUT my_index/_doc/4\n{\n  \"text\": \"Geo-point as an array\",\n  \"location\": [ -71.34, 41.12 ]\n}\n\nPUT my_index/_doc/5\n{\n  \"text\": \"Geo-point as a WKT POINT primitive\",\n  \"location\" : \"POINT (-71.34 41.12)\"\n}\n```\n\n## Querying\n\nNow that we have all this lovely geo data being indexed. How can we query it?\n\nWe are going to cover 2 of the options for querying geo-point data, _geo_distance_ and _geo_bounding_box_.\n\n### Filtering by distance\n\nFiltering by distance will only return documents that occur within a specific distance from a geo-point.\n\n```json\nGET /my_index/_search\n{\n    \"query\": {\n        \"bool\" : {\n            \"must\" : {\n                \"match_all\" : {}\n            },\n            \"filter\" : {\n                \"geo_distance\" : {\n                    \"distance\" : \"200km\",\n                    \"location\" : {\n                        \"lat\" : 40,\n                        \"lon\" : -70\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\nAs with adding documents to our index, geo-points can be filtered in different representations of a geo-point. These can be seen [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-geo-distance-query.html#_accepted_formats).\n\n### Filtering by a bounding box\n\nUsing a bounding box in our query allows us to filter our documents based on a bounding box. This could range from a bounding box containing a city to one containing a whole hemisphere. Either way the query looks like this:\n\n```json\nGET my_index/_search\n{\n    \"query\": {\n        \"bool\" : {\n            \"must\" : {\n                \"match_all\" : {}\n            },\n            \"filter\" : {\n                \"geo_bounding_box\" : {\n                    \"location\" : {\n                        \"top_left\" : {\n                            \"lat\" : 40.73,\n                            \"lon\" : -74.1\n                        },\n                        \"bottom_right\" : {\n                            \"lat\" : 40.01,\n                            \"lon\" : -71.12\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\nAs with adding documents to our index, geo-points can be filtered in different representations of a geo-point. These can be seen [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-geo-bounding-box-query.html#query-dsl-geo-bounding-box-query-accepted-formats).\n\nThe vertices of the bounding box can also be set to _top_right_ and _bottom_left_, as well as _topLeft_, _topRight_, _bottomLeft_ and _bottomRight_. You can also set the _top_, _bottom_, _left_ and _right_ separately.\n\n## In conclusion\n\nNeed the ability to query shapes like squares and polygons? Check out [geo-shapes](https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-shape.html) and use this [awesome tool](https://geojson.net) to generate your polygons\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know.\n\nCheck out our software focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"},{"slug":"giving-my-first-technical-talk","category":"blog","title":"Giving my first technical talk, the good, the bad and the ugly","description":"I recently had the opportunity to give a talk at a local meetup. Here is how it wnet","tags":["graphql","reactnative","techtalks"],"body":"\n# Giving my first technical talk, the good, the bad and the ugly\n\nI recently had the opportunity to give a talk at a local meetup, [South West Mobile](https://www.meetup.com/swmobile/). I have always feared public speaking, especially to my peers and so set myself a goal this year to try it out. I am discuss the wild mental journey I went through. I appreciate that speaking at a meetup to 45 people is not a big deal some but to me it was huge.\n\nIt all started with CFP (Call for papers) that I had forgotten I had submitted. Once the confusion was cleared up and I had remembered that I had submitted it, I was confirmed. I began planning the talk. The title was \"GraphQL in React Native with AWS AppSync\".\n\n## A quick rundown of tech\n\nReact Native is a technology we use a great deal at Gravitywell. The speed of development it allows is fundamental to producing the ever-important [MVP](https://www.gravitywell.co.uk/insights/how-an-mvp-can-help-your-next-project/) or prototype. It allows us to iterate quickly and easily. But then when we are ready to push the full version it is able to scale and provides us with all the functionality we need to produce enterprise-grade mobile applications.\n\nWe really appreciate the benefits of producing an MVP and getting your product on the market as quickly as possible. If you are interested in learning more about MVPs then check out some of our other articles:\n\n- [How an MVP can help your next project](https://www.gravitywell.co.uk/insights/how-an-mvp-can-help-your-next-project/)\n- [How to build an MVP](https://www.gravitywell.co.uk/insights/how-to-build-an-mvp/)\n- [Minimum Viable Product (MVP) - 5 of the best examples](https://www.gravitywell.co.uk/insights/minimum-viable-product-mvp-5-of-the-best-examples/)\n\nGraphQL is another favourite of ours. GraphQL is a query language that allows you to describe the data we want.\n\nThe final piece from the title is [AppSync](https://aws.amazon.com/appsync/). AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need, give the ability to pull data from multiple data sources in a secure manner.\n\nThats a brief overview of what I discussed in the talk but this article is about the process and my mindset.\n\n## Preparation\n\nTo prepare for the talk, I started with an app, that I would use to help show how great the React Native, GraphQL combo is. I was not concerned that the example was slightly contrived as it was perfect for showing how they work together without confusing things with a complicated app.\n\nThe demo app code can be found [here](https://github.com/jim-at-jibba/sw-mobile-graphql)\n\nI then created the slides. Initially I started this with [MDX Deck](https://github.com/jxnblk/mdx-deck) but this proved to time consuming and I was not able to get the hot reloading working correctly so I resorted back to Google slides. This was definitely a good choice.\n\nI started with setting out the basic structure and then added content where needed. I had heard that you should have roughly 1 slide a minute. The talk was meant to be about 20 mins and I was doing a live demo so I settled on 15 slides.\n\n## The big day - ABSOLUTE TERROR\n\n![Terror](https://media.giphy.com/media/NTSPYhlghwX5e/giphy.gif)\n\nThe day arrived and although I knew I was prepared, I was terrified. My stomach was in knots, I could not eat and I struggled to think straight! But before I knew it, it was 7pm, I was set up and ready to go. I was second on the bill. The first talk was on accessibility, it was very polished and the speaker had ~50 slides. This immediately made me nervous. I only had 15. Was my talk gonna be 3 mins long? Was I gonna get laughed out of the room?\n\nThen my turn came.\n\nThankfully, I was not laughed at. The talk got a few laughs and I did not pass out. My live coding demo did break, right at the last moment (I later found it was due to miss spelling a GraphQL type) but that was fine. On the whole it felt like it went ok but I had already decided in my head that I was not gonna do another. I had found the whole experience really stressful and time consuming. With my mind made up, I went home happy that I had done it.\n\nBut the next day, the recording of the talks was released. I tentatively watch my session and I was pleasantly surprised, it was much better than I was expecting. On the outside I was as calm as a Hindu cow but I knew that on the inside I was screaming! But this gave me the strange feeling that I should do another. My wife said the same thing! She was trying to get me to sign up to do another one straight away.\n\nNow I am confused, should I do another? Will it be easier? Will it be less terrifying?\n\nAll of these can be answered with \"probably\" and I wont find out until I try again. But that is where I have left it. I am still deciding if I want to put myself through it again even though it will probably be easier!\n\nWatch this space.\n\n## A few takeaways\n\n- If like me, you are scared of public speaking. Start at a meetup with a topic you know really well. It should make it a little easier. I was really lucky that SouthWest mobile were a really lovely bunch of humans.\n- If you are not first, try not to worry about what the previous speaker is doing. Generally your talks will be very different so it should not really matter.\n- If you do decide to do one and it bombs, try not to worry about it, don't listen to people if they are being critical. Stuff like this happens all the time. At least you stepped up and gave it a go. There is a fantastic quote from Teddy Roosevelt that puts this better than I could\n\n> It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly...\n\n[Slides](https://speakerdeck.com/jimgbest/graphql-in-react-native-with-aws-appsync)\n"},{"slug":"ios-project-setup-programatic","category":"blog","title":"Setting up an Xcode Project for Programmatic UIs","description":"Setting up an Xcode Project for Programmatic UIs","tags":["mobile","ios"],"body":"\nWhen it comes to developing iOS applications, Xcode is the go-to integrated development environment (IDE) for many developers. Xcode provides a comprehensive set of tools and features that simplify the process of building robust and user-friendly apps. One of the key decisions developers often face is whether to use storyboards or go completely programmatic for their user interfaces. In this article, we will explore the process of setting up an Xcode project for programmatic projects, allowing developers to create user interfaces entirely in code.\n\nThe first step in setting up a programmatic project is to remove the default main.storyboard file that Xcode creates when you start a new project. To do this, simply locate the file in the project navigator and delete it. Don't worry, removing the storyboard file doesn't mean you won't be able to create a user interface‚Äîit just means you'll be doing it programmatically.\n\nOnce the main.storyboard file has been deleted, navigate to the Info tab of your project settings. Look for the Main storyboard file base name entry and delete it. This step ensures that Xcode won't try to load a storyboard file at runtime.\n\nNext, let's make sure we remove any references to storyboards in the project configuration. Use the search functionality in Xcode to search for the term 'storyboard.' This search will help us locate any remaining references to storyboards that need to be removed. One important entry to find and delete is the UISceneStoryboardFile entry. Removing this entry prevents Xcode from trying to load a storyboard file to instantiate the app's initial scene.\n\nNow that we have removed the necessary storyboard-related configurations, we need to make some changes to the SceneDelegate.swift file. Open the file and locate the scene(\\_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) function. This function is called when a new scene is being created for the app.\n\nInside the scene function, we need to add a few lines of code to set up the initial scene programmatically. This code will create a new window, set the window's scene to the provided windowScene, assign a root view controller, and make the window visible. Here's the code snippet you should add to the scene function:\n\n```swift\nfunc scene(_ scene: UIScene, willConnectTo session: UISceneSession, options connectionOptions: UIScene.ConnectionOptions) {\n    guard let windowScene = (scene as? UIWindowScene) else { return }\n\n    // Add these bits\n    window = UIWindow(frame: windowScene.coordinateSpace.bounds)\n    window?.windowScene = windowScene\n    window?.rootViewController = ViewController()\n    window?.makeKeyAndVisible()\n}\n```\n\nThe added code creates a new instance of UIWindow using the provided windowScene as its frame. It then assigns the window scene to the created window and sets the root view controller to an instance of ViewController(). Finally, the makeKeyAndVisible() function ensures that the window becomes visible.\n\nWith these changes in place, you are now ready to start building your programmatic user interface. The ViewController() class used in the example code is a placeholder for your custom view controller class. You should replace it with your own implementation or create a new view controller class that suits your project's needs.\n\nSetting up an Xcode project for programmatic projects provides developers with greater control and flexibility over their user interfaces. By removing the default storyboard and making the necessary adjustments to the project configuration and scene delegate, developers can fully embrace a programmatic approach to UI development. Whether you prefer a purely code-based workflow or want to mix storyboards with programmatic views, Xcode offers the versatility to cater to your preferences and project requirements. Happy coding!\n"},{"slug":"j5-repl","category":"blog","title":"Javascript robotics: The Johnny Five REPL","description":"Lets interact with your Javascript robots","tags":["robotics","javascript","esp8266"],"body":"\nIn the [previous article](https://jamesbest.tech/posts/wireless-javascript-robots-johhny-five-esp8266), we discussed how to get up and running with Johnny Five and the ESP8266 micro-controller. If you are not sure how to get set up I recommend you read that article first.\n\nPreviously we created a simple script that allowed us to control the onboard LED with our keyboard. This required us to install the `keypress` package and listen in `process.stdin`. This is not a bad approach but there is a better one.\n\nJohnny Five comes packaged with a REPL. A REPL (read-eval-print-loop) is an interactive language shell giving a simple interactive interface to the Johnny Five API. We can control our robots with the REPL but there is some set up first.\n\n## Setting up the REPL\n\nWe need to make the REPL aware of our hardware by injecting the `instance` of the hardware into our script. We are using the script from before but with the keyboard code stripped out:\n\n```javascript\nconst { EtherPortClient } = require(\"etherport-client\")\nconst { Board, Led, Pin } = require(\"johnny-five\")\n\nconst board = new Board({\n  port: new EtherPortClient({\n    host: \"192.168.1.109\",\n    port: 3030,\n  }),\n  repl: false,\n})\n\nconst LED_PIN = 2\n\nboard.on(\"ready\", () => {\n  console.log(\"Board ready\")\n  var led = new Led(LED_PIN)\n})\n```\n\nNow, let's add the REPL code. Update the board ready callback to look like this:\n\n```javascript\n[...]\nboard.on(\"ready\", function() {\n  /*\n    Initialize pin 2, which\n    controls the built-in LED\n  */\n  var led = new Led(LED_PIN);\n\n  /*\n    Injecting object into the REPL\n    allow access while the program\n    is running.\n\n    Try these in the REPL:\n\n    led.on();\n    led.off();\n    led.blink();\n\n  */\n  board.repl.inject({\n    led: led\n  });\n});\n```\n\nWith this simple addition, we now have access to all the functions available on the LED object.\n\n## Limiting access\n\nBut what if we don't want to give our users unfettered access? What if we only want to give access to specific functions or write functions that do more than just control the hardware. Maybe we want to add logging or provide more appropriate function names. Well, we can write our own functions that are available in the REPL and inject those instead.\n\n```javascript\n[...]\nboard.on(\"ready\", function() {\n  /*\n    Initialize pin 2, which\n    controls the built-in LED\n  */\n  var led = new Led(LED_PIN);\n\n  board.repl.inject({\n    // Allow limited on/off control access to the\n    // Led instance from the REPL.\n    on: function() {\n      led.on();\n    },\n    off: function() {\n      led.off();\n    },\n    flash: function () {\n      led.blink();\n    },\n\n  });\n});\n```\n\nThis script will make `on()`, `off()` and `flash()` functions available in the REPL.\n\nAnd that is it. A nice, short intro to the Johnny Five REPL. Until you start hooking your scripts up to WebSockets or a REST API, I think this is one of the better ways to control your robots.\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know.\n\n## Check out our software-focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"},{"slug":"meeting-lights","category":"blog","title":"Avoiding that awkward BBC interview moment with Google Calendar, IFTTT, Tasker and a Raspberry Pi.","description":"During lockdown I needed to find a simple way to let my family know when I was in a meeting. Here was how I solved it.","tags":["remoteworking","webdev"],"body":"\n![Interview](https://media.giphy.com/media/ZdU3bTTc1WWStZM5lm/giphy.gif)\n\nHaving experienced this in my first week of working at home I knew I needed a solution for letting my wife and kid know when I was going to be in a meeting/pair programming. I could just shout downstairs but where is the fun in that? By automating it, it is also one less thing to have to remember in what is shaping up to be a stressful time for everyone.\n\nI wanted this to be a quick Sunday evening project that only took a couple of hours so needed it to be simple and use the kit I already had. I also wanted it to work without the Raspberry Pi (RPi) being accessible from outside my home network.\n\nI could have completed this project in a much simpler way if I had allowed access to the RPi from the outside world but in a world where IoT devices are the target of cyber attacks I wanted to limit the exposure.\n\nOn the hardware side of things, it seemed obvious to use an RPi and [Mote](https://shop.pimoroni.com/collections/mote) LEDs. I have them already and they are simple to work with.\n\nMote is a collection of components allowing you to easily work with LEDs. The visual side of this project could be anything you have knocking around. I just happened to have these.\n\nI am using an RPi 3 for this but you could any Pi that has is network connected.\n\nAll my meetings are in Google Calendar so I just needed a way to trigger an event when my meetings start and finish. _If This Than That_ (IFTTT) is perfect for this. Google Calendar is the _THIS_ of IFTTT and a webhook is the _THAT_.\n\nI wanted this to be a quick project with as little programming as possible so I needed a way to accept the webhook but also be able to perform actions on a device in my local network. This part took a little longer to figure out, but a combination of [Tasker](https://www.androidcentral.com/tasker) and [AutoRemote](https://www.pocketables.com/2012/09/beginners-guide-to-tasker-part-6-autoremote.html) was able to perfectly meet my needs.\n\n**Tasker** is an automation app that allows users to perform actions based on an event.\n\n**AutoRemote** allows devices to communicate with each other without any user interaction.\n\n_N.B This article assumes some previous knowledge of Tasker and AutoRemote and so the instructions may be brief_\n\nTasker and AutoRemote are built to work with each other and so we can use AutoRemote as a way of triggering an action in Tasker. AutoRemote is available as a plugin in Tasker.\n\nAutoRemote acts as our \"server\" for the IFTTT webhook to callout to. When it receives a request, an event is triggered in Tasker which in turn sends a REST request to a local flask app that is running on the RPi. This Flask app controls the Mote LEDs.\n\n## Setting up Tasker and AutoRemote\n\nIn AutoRemote we need to register IFTTT as a device. Click the phone icon in the tab bar and then click the IFTTT logo and follow the instructions. This will give you a URL that can be added to the IFTTT webhook. It will look something like this:\n\n```bash\nhttps://autoremotejoaomgcd.appspot.com/sendmessage?key=[***yourKey***]&sender=[***yourId***]&message=IFTTT_MEETING_BUSY=:=true\n```\n\nNotice `message=IFTTT_MEETING_BUSY=:=true` this is how we pass properties to AutoRemote. The left side of **=:=** is how we can filter incoming requests and the right side are any params you want to pass in.\n\nWith this set up we now need to trigger events in Tasker. Under the _profiles_ tab create a new one profile selecting _event -> plugin -> AutoRemote -> AutoRemote_. Edit the configuration and add the name of the event (the left side of the request from IFTTT e.g IFTTT*MEETING_BUSY). Next, click back and add the action. In my case, I wanted to perform an HTTP request* to my RPi but there is a multitude of things that Tasker can do for you.\n\nOnce set up your profiles will look like this\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/i7xyimaq0uv0vpnnpiys.jpg)\n\nThe final step is setting up the local server to trigger the Mote lights. I wrote a really simple Flask app that listens to requests and calls the correct Mote methods. It looks something like this:\n\n```python\nimport motephat\nfrom flask import Flask, jsonify, make_response\n\napp = Flask(__name__)\n\nmotephat.set_brightness(0.5)\nmotephat.configure_channel(1, 16, False)\n#motephat.configure_channel(2, 16, False)\n#motephat.configure_channel(3, 16, False)\n#motephat.configure_channel(4, 16, False)\n\ncolour = 'FFFFFF'\nstatus = 0\n\ndef mote_on(c):\n    for channel in range(1, 5):\n            for pixel in range(16):\n                motephat.set_pixel(channel, pixel, 255, 0, 0)\n    motephat.show()\n\n    return True\n\ndef mote_off():\n    motephat.clear()\n    motephat.show()\n    return True\n\ndef get_status():\n    global status\n    for channel in range(4):\n        for pixel in range(16):\n            if motephat.get_pixel(channel + 1, pixel) != (0, 0, 0):\n                status = 1\n    return status\n\n@app.route('/')\ndef hello():\n    return 'hello'\n\n@app.route('/mote/api/v1.0/<string:st>', methods=['GET'])\ndef set_status(st):\n    global status, colour\n    if st == 'on':\n        status = 1\n        mote_on(colour)\n    elif st == 'off':\n        status = 0\n        mote_off()\n    elif st == 'status':\n        status = get_status()\n    return jsonify({'status': status, 'colour': colour})\n\n@app.errorhandler(404)\ndef not_found(error):\n    return make_response(jsonify({'error': 'Not found'}), 404)\n\nif __name__ == '__main__':\n    mote_off()\n    app.run(debug='true', host='0.0.0.0')\n\n```\n\nNow when you are about to go into a meeting your lights should look like this\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/uyj69l2f29wecocywkmk.jpg)\n\n## Conclusion\n\nI hope this was of some interest. I really enjoy these little projects that have limited time and scope. Obviously, there are so many ways that this could be done and some many improvements that could be made but this was only meant to be a little bit of fun on a Sunday night.\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know.\n\n## Check out our software-focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"},{"slug":"react-native-android-error","category":"blog","title":"Android build failure due to SDK version mismatch","description":"Fixing a random Android build error when we had not updated any dependencies","tags":["react-native","android","bugs"],"body":"\nReact Native is a popular framework for developing mobile applications using JavaScript and React. It allows developers to build apps for both iOS and Android platforms using a single codebase. However, like any software development process, React Native projects can encounter issues and errors that need to be resolved. One such problem is the occurrence of random build failures on Android due to a mismatch in the minCompiledSDK version.\n\n### The Error\n\nA random Android build failure occurred in our React Native project, and upon investigation, we discovered that it was caused by a mismatch in the minCompiledSDK version. The error message indicated that the androidx.browser package had been updated the previous day with a higher minCompiledSDK, leading to the failure.\n\nThe specific error message looked like this:\n\n```bash\n[...]\nThe minCompileSdk (34) specified in a\ndependency's AAR metadata (META-INF/com/android/build/gradle/aar-metadata.properties)\nis greater than this module's compileSdkVersion (android-33).\nDependency: androidx.browser:browser:1.6.0-beta01.\n[...]\n```\n\n### The Fix\n\nTo resolve this issue, we needed to ensure that the androidx.browser package uses a compatible minCompiledSDK version with our project's compileSdkVersion. We made the necessary adjustments in the build.gradle file to force the usage of a specific version.\n\nHere is the modified build.gradle file:\n\n```gradle\n[...]\ndependencies {\n   def work_version = \"1.5.0\"\n    // Force androidx.browser:browser: 1.5.0 for transitive dependency\n    implementation(\"androidx.browser:browser:$work_version\") {\n        force = true\n    }\n}\n[...]\n```\n\nBy specifying the desired version of the androidx.browser package and setting force to true, we ensured that the build system uses the compatible version regardless of any metadata discrepancies. In this case, we used version 1.5.0 to match our project's compileSdkVersion of android-33.\n\n## Conclusion\n\nRandom build failures can be frustrating, especially when they occur due to seemingly unrelated package updates. In the case of a minCompiledSDK mismatch in React Native Android builds, ensuring compatibility between dependencies and the project's compileSdkVersion is crucial. By using the force option in the build.gradle file, we can override any conflicting metadata and specify the desired version for the problematic package.\n\nIt is important to regularly review and update dependencies in a React Native project to avoid potential compatibility issues. Additionally, staying informed about the latest updates and changes in packages and libraries can help prevent unexpected build failures and streamline the development process.\n"},{"slug":"running-python-on-boot","category":"blog","title":"Running a python script on boot on a RPi","tags":["maker","python","raspberry-pi"],"body":"\nOnce you are able to successfully run your python script from the terminal, what are your options to run the same script on booting the Raspberry Pi? This post details the issues I had getting my scripts to run on boot and how I fixed them.\n\n**TL;DR** Make sure you are running your scripts as the correct user. Python scripts that use Pip packages will need to be run as the user used to install said packages or else they might not be available.\n\nWriting python scripts is a relatively new thing for me. I love playing with the Raspberry Pi and generally write Nodejs. For most of the stuff I have been doing up until this point, Nodejs has been fine. But the Raspberry Pi is blessed with a wealth of hats performing a multitude of weird and wonderful things. The problem is that most of the libraries for these hats are written in Python. So I thought it was about time I start learning some python.\n\nMy first python script was controlling the Mote LEDs through MQTT. Writing the script did not take too long as the examples in the Pimoroni repos gave me enough to get started. I was able to run the script from the terminal and had it working fine. Obviously, we don't want to have a terminal session open all the time and we want to have the script start on boot (or reboot).\n\nI had read that there are a number of ways to run a script on boot but that the preferred way was to add an entry into the `/etc/rc.local` file. This file is loaded when the Pi is booted and so should achieve our goal.\n\nHave you ever seen the IP of the Pi print in the terminal output when the Pi is booting? This is where that command is run from.\n\nSo I added the following line to the file:\n\n```bash\n/usr/bin/python3 /home/pi/Code/desklights/desklights.py\n```\n\nand rebooted the Pi. Nothing! No logs, no error. Nothing! Very unhelpful. I tried every possible way of calling the script. And each one proved fruitless. Nothing.\n\nWhat I needed was to see so sort of log output. I hoped that this would give me an idea of what was wrong. I updated the command to the following:\n\n```bash\n/usr/bin/python3 /home/pi/Code/desklights/desklights.py > /tmp/desklights.out > /tmp/desklights.err\n```\n\nThis would write anything logging from the program to `/tmp/desklights.out` and and errors to `/tmp/desklights/err`. This was a key to solving the issue. The _.out file was empty which indicated the script was failing. The `_.err` file, on the other hand, contained this:\n\n![Alt Text](https://thepracticaldev.s3.amazonaws.com/i/3g93i5dwvjay78yito7z.png)\n\nIt was now clear what I needed to do. The python packages I had installed were install using the Pi user. This file runs as root and so does not have access to the installed packages. I need to make sure I am running my script as the Pi user and all should be fine and dandy.\n\n```bash\nsudo -H -u pi /usr/bin/python3 /home/pi/Code/desklights/desklights.py/home/pi/Code/desklights/desklights.py\n```\n\nUpdating the script to include the change of user fixed my issue and allowed the script to run successfully on boot.\n\nThanks for reading üôè\n"},{"slug":"wireless-js-robotics-johnny-five","category":"blog","title":"Wireless javascript robotics with Johnny five and the ESP8266","description":"Want to build robots with Javscript?","tags":["robotics","javascript","esp8266"],"body":"\nIn this article, I will talk about the first steps needed to start building robots with Javascript. I will be using the infamous [ESP8266 microcontroller](https://en.wikipedia.org/wiki/ESP8266), this is because it is super cheap and allows it not to be tethered to your machine in the way that an Arduino would be.\n\nTo allow us to write our robotics scripts in Javascript with will be using the [Johnny-Five](http://johnny-five.io/) library written by Rick Waldron. The library supports a huge selection of boards and hardware. Although not all boards support all of the hardware.\n\nSeveral other boards could be used as alternatives (Proton, Tessel) but these are considerable more expensive and not as easily available. It is also possible to use an Arduino attached to a Raspberry Pi (RPi) and you then interface wirelessly with the RPi but that seems a little unnecessary now.\n\nThere are better languages to build robots with, but being an engineer working primarily in Javascript, I was keen to keep things close to home. In this setup Javascript is not run on the microcontroller but is run through some custom firmware called the Firmata protocol. This does mean it runs slower than something like C would but generally for things like robots this is not much of an issue. The first step in this process is to upload the `StandardFirmataWifi` sketch onto our board but to be able to do that we need the Arduino IDE and for it to recognise our ESP8266 based boards.\n\n## Getting set up\n\nThe following instructions are based on using a mac. They will be very similar to other platforms.\n\nCopy the following URL `http://arduino.esp8266.com/stable/package_esp8266com_index.json`. Open the IDE and go to the files menu and click on preferences. Add the URL to the ‚ÄòAdditional Boards Manager URL‚Äô.\n\n![Image of arduino ide](/images/posts/j5-esp/Addingesp8266.png)\n\nClose the preferences panel and click tools. Under tools select boards and then board manager. Navigate to esp8266 by esp8266 community and install the software for Arduino.\n\n![Board Manager](/images/posts/j5-esp/board-manager-open.png)\n\nOnce this has been done you should be able to program your ESP8266 based board, I am using the NodeMCU board.\n\nClick the examples panel and select the Wireless Firamta sketch. We will be updating so make a copy now.\n\nWe need to update the header file `wifiConfig.h` with our network credentials. Update the values of `char ssid[] =\"\"` and `char wpa_passphrase[] =\"\"`.\n\nWith this done we can now upload the sketch to our device. Once uploaded you can close the Arduino IDE.\n\nDon‚Äôt forget that the ESP\\* boards have a different pin layout to the Arduino boards. See the picture below for an example.\n\n![Board Manager](/images/posts/j5-esp/NodeMCUPinout.png)\n\nNow that we are all set up on the microcontroller we need to create a new node project and install the needed packages.\n\n## Our first robot script\n\nCreate a new folder for the project and initialise a new node project\n\n`mkdir helloWorld && cd $_ && npm init -y`\n\nNow we need to install Johnny-Five and the ethernet client that allows us to connect wirelessly.\n\n`npm install johnny-five ethernet-client keypress`\n\nWith this done we are set to write our first script.\n\n```javascript\nconst { EtherPortClient } = require(\"etherport-client\")\nconst { Board, Led, Pin } = require(\"johnny-five\")\nconst keypress = require(\"keypress\")\n\nconst board = new Board({\n  port: new EtherPortClient({\n    host: \"192.168.1.109\",\n    port: 3030,\n  }),\n  repl: false,\n})\n\nkeypress(process.stdin)\nconst LED_PIN = 2\n\nboard.on(\"ready\", () => {\n  console.log(\"Board ready\")\n  var led = new Led(LED_PIN)\n  console.log(\"Use Up and Down arrows for On and Off. Space to stop.\")\n\n  process.stdin.resume()\n  process.stdin.setEncoding(\"utf8\")\n  process.stdin.setRawMode(true)\n\n  process.stdin.on(\"keypress\", (ch, key) => {\n    if (!key) {\n      return\n    }\n\n    if (key.name === \"q\") {\n      console.log(\"Quitting\")\n      process.exit()\n    } else if (key.name === \"up\") {\n      console.log(\"Blink\")\n      led.blink()\n    } else if (key.name === \"down\") {\n      console.log(\"Stop blinking\")\n      led.stop()\n    }\n  })\n})\n```\n\nWe need to replace `10.0.0.49` with the IP assigned to our board. I use an app called Fing but this information can be found out from the Serial monitor in the Arduino IDE.\n\n![Fing](/images/posts/j5-esp/fing.jpg)\n\nThis simple script will make the allow you to turn the onboard led on and off. Nothing fancy but paves the way for much more exciting things. To execute the file `node index.js`. You should see something similar to this:\n\n```bash\n$ node hello.js\n1590554783332 SerialPort Connecting to host:port: 192.168.1.109:3030\n1590554783334 Connected Connecting to host:port: 192.168.1.109:3030\n1590554793338 Use Up and Down arrows for On and Off. Space to stop.\n```\n\nNow that the board is set up we are ready to create some more interesting projects. Johnny Five provides such a gentle introduction to robotics in Javascript but allows you to do so much as you have the wealth of packages in NPM and hundreds of public APIs that you can lean on to create great projects.\n\nThe next article will be a nice short one, introducing the Johnny Five repl and why it is great for prototyping your next project.\n\n# Thanks for reading üôè\n\nIf there is anything I have missed, or if there is a better way to do something then please let me know.\n\nCheck out our software focused podcast - [Salted Bytes](https://open.spotify.com/show/7IdlgpiDfYcOdCn57mPLvH?si=X1ArfHvqQXSOAfc1h7Y_Eg)\n"}]